{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-19T16:45:02.893572703Z",
     "start_time": "2023-11-19T16:45:02.708774591Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from multiprocessing import Pool\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle as pkl\n",
    "from omnibelt import load_json, save_json\n",
    "from nltk.corpus import stopwords\n",
    "from itertools import combinations, islice\n",
    "from collections import Counter\n",
    "import unicodedata\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "54"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang_clusters = {'en': ['au', 'ca', 'gb', 'ie', 'in', 'my', 'ng', 'nz', 'ph', 'sa', 'sg', 'us', 'za'],\n",
    "\t\t\t\t 'es': ['ar', 'co', 'cu', 'mx', 've'], 'de': ['at', 'ch', 'de'], 'fr': ['be', 'fr', 'ma'],\n",
    "\t\t\t\t 'zh': ['cn', 'hk', 'tw'], 'ar': ['ae', 'eg'], 'pt': ['br', 'pt'], 'bg': ['bg'], 'cs': ['cz'],\n",
    "\t\t\t\t 'el': ['gr'], 'he': ['il'], 'hu': ['hu'], 'id': ['id'], 'it': ['it'], 'ja': ['jp'], 'ko': ['kr'],\n",
    "\t\t\t\t 'lt': ['lt'], 'lv': ['lv'], 'nl': ['nl'], 'no': ['no'], 'pl': ['pl'], 'ro': ['ro'], 'ru': ['ru'],\n",
    "\t\t\t\t 'sv': ['se'], 'sl': ['si'], 'sk': ['sk'], 'sr': ['rs'], 'th': ['th'], 'tr': ['tr'], 'uk': ['ua']}\n",
    "loc_names = {'gb': 'United Kingdom', 'ar': 'Argentina', 'pl': 'Poland', 'sk': 'Slovakia', 'us': 'United States',\n",
    "\t\t\t 'eg': 'Egypt', 'no': 'Norway', 'ph': 'Philippines', 'at': 'Austria', 'rs': 'Serbia', 'tw': 'Taiwan',\n",
    "\t\t\t 'be': 'Belgium', 'cu': 'Cuba', 'sa': 'Saudi Arabia', 'th': 'Thailand', 'id': 'Indonesia',\n",
    "\t\t\t 'ru': 'Russian Federation', 'ch': 'Switzerland', 'fr': 'France', 'lt': 'Lithuania', 'tr': 'Turkey',\n",
    "\t\t\t 'de': 'Germany', 'cz': 'Czechia', 'pt': 'Portugal', 'ae': 'United Arab Emirates', 'it': 'Italy',\n",
    "\t\t\t 'cn': 'China', 'lv': 'Latvia', 'nl': 'Netherlands', 'hk': 'Hong Kong', 'ca': 'Canada', 'br': 'Brazil',\n",
    "\t\t\t 'hu': 'Hungary', 'kr': 'Korea', 'si': 'Slovenia', 'au': 'Australia', 'my': 'Malaysia', 'ie': 'Ireland',\n",
    "\t\t\t 'ua': 'Ukraine', 'in': 'India', 'ma': 'Morocco', 'bg': 'Bulgaria', 'ng': 'Nigeria', 'il': 'Israel',\n",
    "\t\t\t 'se': 'Sweden', 'za': 'South Africa', 've': 'Venezuela', 'nz': 'New Zealand', 'jp': 'Japan',\n",
    "\t\t\t 'sg': 'Singapore', 'gr': 'Greece', 'mx': 'Mexico', 'co': 'Colombia', 'ro': 'Romania'}\n",
    "lang_names = {'en': 'English', 'ko': 'Korean', 'ru': 'Russian', 'es': 'Spanish', 'pt': 'Portuguese', 'cs': 'Czech',\n",
    "\t\t\t  'tr': 'Turkish', 'nl': 'Dutch', 'ar': 'Arabic', 'fr': 'French', 'bg': 'Bulgarian', 'id': 'Indonesian',\n",
    "\t\t\t  'sk': 'Slovak', 'el': 'Greek', 'he': 'Hebrew', 'sr': 'Serbian', 'hu': 'Hungarian', 'th': 'Thai',\n",
    "\t\t\t  'zh': 'Chinese', 'no': 'Norwegian', 'sl': 'Slovenian', 'sv': 'Swedish', 'de': 'German', 'lv': 'Latvian',\n",
    "\t\t\t  'pl': 'Polish', 'it': 'Italian', 'ro': 'Romanian', 'lt': 'Lithuanian', 'ja': 'Japanese',\n",
    "\t\t\t  'uk': 'Ukrainian'}\n",
    "cluster_id = {loc: lang for lang, locs in lang_clusters.items() for loc in locs}\n",
    "len(cluster_id)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T16:45:02.945041519Z",
     "start_time": "2023-11-19T16:45:02.901181060Z"
    }
   },
   "id": "b55370e4437f92ec"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def topk_ngrams(importances, ordered_keywords, n=2, k=10):\n",
    "\tassert len(ordered_keywords) >= n, f'Not enough tokens in bag: {len(ordered_keywords)} < {n}: {ordered_keywords}'\n",
    "\treturn [frozenset(ws) for ws in islice(\n",
    "\t\tsorted(combinations(ordered_keywords, n), key=lambda ws: sum(importances[w] for w in ws), reverse=True), k)]\n",
    "# def ngram_shells(importances, keywords, num_shells=3, k=10):\n",
    "# \tmost = min(num_shells, len(keywords))\n",
    "# \tcontent = [set(topk_ngrams(importances, keywords, n=i, k=k)) for i in range(1, most + 1)]\n",
    "# \tfor _ in range(most, num_shells):\n",
    "# \t\tcontent.append(set())\n",
    "# \treturn content\n",
    "def has_hits(art_keywords, aids, segs):\n",
    "\treturn any(all(w in art_keywords[aid] for w in seg) for aid in aids for seg in segs)\n",
    "def find_hits(art_keywords, aids, segs):\n",
    "\thits = {}\n",
    "\tfor aid in aids:\n",
    "\t\tfor seg in segs:\n",
    "\t\t\tif all(w in art_keywords[aid] for w in seg):\n",
    "\t\t\t\thits.setdefault(seg, []).append(aid)\n",
    "\treturn hits\n",
    "def segs2keywords(importances, segs):\n",
    "\treturn sorted(set(w for seg in segs for w in seg), key=lambda w: importances[w], reverse=True)\n",
    "def generate_candidates(art_keywords, importances, tier, kws, members=None, num_kw=10, num_member=10):\n",
    "\tcands = set()\n",
    "\tif len(kws) >= tier:\n",
    "\t\tcands.update(topk_ngrams(importances, kws, n=tier, k=num_kw))\n",
    "\tif members and len(members) and num_member:\n",
    "\t\tmember_cands = Counter()\n",
    "\t\tfor aid in members:\n",
    "\t\t\tmember_cands.update(topk_ngrams(importances, art_keywords[aid], n=tier, k=num_kw))\n",
    "\t\tfor seg, _ in member_cands.most_common(num_member):\n",
    "\t\t\tcands.add(seg)\n",
    "\treturn cands\n",
    "def mainline_cluster_tiers(art_keywords, importances, center, options, starting_tier=7, min_tier=2, num_kw=10, num_member=10):\n",
    "\tkws = art_keywords[center]\n",
    "\tmembers = set()\n",
    "\n",
    "\tknown_tokens = set()\n",
    "\ttokens = {}\n",
    "\ttiers = {}\n",
    "\ttrace = Counter()\n",
    "\tremaining = list(options)\n",
    "\tif center in remaining:\n",
    "\t\tremaining.remove(center)\n",
    "\tassert min_tier >= 1, f'Minimum tier must be at least 1: {min_tier}'\n",
    "\tfor tier in range(starting_tier, min_tier - 1, -1):\n",
    "\t\tif not len(remaining):\n",
    "\t\t\tbreak\n",
    "\n",
    "\t\tcands = generate_candidates(art_keywords, importances, tier, kws, members=members, num_kw=num_kw, num_member=num_member)\n",
    "\t\t# print(tier, len(cands))\n",
    "\t\ttrace[tier] = len(cands)\n",
    "\t\tif not has_hits(art_keywords, remaining, cands):\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\thits = find_hits(art_keywords, remaining, cands)\n",
    "\n",
    "\t\tgold = {}\n",
    "\t\tfor seg, aids in hits.items():\n",
    "\t\t\tfor aid in aids:\n",
    "\t\t\t\tgold.setdefault(aid, set()).update(seg)\n",
    "\t\ttokens[tier] = Counter(w for seq in gold.values() for w in seq if w not in known_tokens)\n",
    "\t\tkws = segs2keywords(importances, hits.keys())\n",
    "\t\tknown_tokens.update(kws)\n",
    "\t\ttiers[tier] = set(aid for seg, aids in hits.items() for aid in aids)\n",
    "\t\tmembers.update(tiers[tier])\n",
    "\t\tremaining = [aid for aid in remaining if aid not in members]\n",
    "\n",
    "\treturn tokens, tiers, trace\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(\"'s\", '|', 'I', \"n't\", \"`s\", \"'s\", 'n`t')\n",
    "_my_stop_words = {\"'s\", 'news'}\n",
    "\n",
    "\n",
    "def is_good_word(w):\n",
    "\treturn any(not unicodedata.category(char).startswith('P') for char in\n",
    "\t\t\t   w) and w not in stop_words and w not in _my_stop_words"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T16:45:06.367672244Z",
     "start_time": "2023-11-19T16:45:06.347170770Z"
    }
   },
   "id": "62d424d5195dcc3a"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/54 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "702abb2356ea4b2d9d7c8912b042d0a4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "4719199"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root = Path('/home/fleeb/workspace/local_data/nnn')\n",
    "recs = (root / 'babel-briefings-v1').glob('**/*.json')\n",
    "recs = list(recs)\n",
    "len(recs)\n",
    "articles = []\n",
    "for rec in tqdm(recs):\n",
    "\tarticles.extend(load_json(rec))\n",
    "len(articles)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T16:45:54.088664564Z",
     "start_time": "2023-11-19T16:45:07.450659113Z"
    }
   },
   "id": "1c3b47853ca85342"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/4719199 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ad88ae40d1824fec85f0fb5269fde5f1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "(54, 30)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "by_loc = {}\n",
    "by_lang = {}\n",
    "by_source = {}\n",
    "by_ID = {}\n",
    "for article in tqdm(articles):\n",
    "\tby_ID[article['ID']] = article\n",
    "\t# article['published'] = parser.parse(article['publishedAt'])\n",
    "\tby_source.setdefault(article['source-name'], []).append(article)\n",
    "\tby_lang.setdefault(article['language'], []).append(article)\n",
    "\tfor instance in article['instances']:\n",
    "\t\t# instance['collected'] = parser.parse(instance['collectedAt'])\n",
    "\t\tby_loc.setdefault(instance['location'], []).append(article)\n",
    "len(by_loc), len(by_lang)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T16:45:58.065008302Z",
     "start_time": "2023-11-19T16:45:54.090384181Z"
    }
   },
   "id": "f85fcd317346520f"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "cluster_root = root / 'clusterings'\n",
    "cluster_root.mkdir(exist_ok=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T16:45:58.109146567Z",
     "start_time": "2023-11-19T16:45:58.066165730Z"
    }
   },
   "id": "f29c546c6ad904a0"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "(4719199, 4719199)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lowercase_all = True\n",
    "# full_bagowords_inds = {int(ID): {k.lower() if lowercase_all else k:v for k,v in bag.items()} \n",
    "# \t\t\t\t\t   for ID, bag in tqdm(load_json(root/'bagowords-ordered-full.json').items())}\n",
    "# \n",
    "# pre_dash = Counter()\n",
    "# for ID, bagi in tqdm(full_bagowords_inds.items()):\n",
    "# \tif '-' in bagi:\n",
    "# \t\tpre_dash.update({w: len(inds) for w, inds in bagi.items() if all(i < bagi[\"-\"][-1] for i in inds)})\n",
    "# \n",
    "# post_dash = Counter()\n",
    "# for ID, bagi in tqdm(full_bagowords_inds.items()):\n",
    "# \tif '-' in bagi:\n",
    "# \t\tpost_dash.update({w: len(inds) for w, inds in bagi.items() if w not in pre_dash})\n",
    "# all_bags_inds = {ID: {w: [i for i in inds if bag.get('-', [float('inf')])[-1] > i]\n",
    "# \t\t\t\t\t  for w, inds in bag.items() if w not in post_dash and is_good_word(w)}\n",
    "# \t\t\t\t for ID, bag in tqdm(full_bagowords_inds.items())}\n",
    "# all_bags = {ID: Counter({w: len(inds) for w, inds in bag.items() if len(inds)}) for ID, bag in all_bags_inds.items()}\n",
    "# len(all_bags)\n",
    "all_bags_inds = pkl.load(open(root/'temp'/'all_bags_inds.pkl', 'rb'))\n",
    "all_bags = {ID: Counter({w: len(inds) for w, inds in bag.items() if len(inds) and is_good_word(w)}) for ID, bag in all_bags_inds.items()}\n",
    "len(all_bags), len(all_bags_inds)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T16:47:12.805131946Z",
     "start_time": "2023-11-19T16:45:58.108757463Z"
    }
   },
   "id": "2d7caae5bdf55d04"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/4719199 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f22049c8ef9045cda4ae53f9c1e681ca"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame(\n",
    "\t[{**inst, 'aid': art['ID']} for art in tqdm(articles) for i, inst in\n",
    "\t enumerate(by_ID[art['ID']]['instances'])])\n",
    "df['collectedAt'] = pd.to_datetime(df['collectedAt'])\n",
    "df = df.sort_values('collectedAt')\n",
    "df['date'] = df['collectedAt'].dt.date"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T16:47:22.685384369Z",
     "start_time": "2023-11-19T16:47:12.792774901Z"
    }
   },
   "id": "f70c4174b1f8e681"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/413 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c0c38953fa3d42fc84544433ae6c88e8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "413"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daybags = {}\n",
    "date_aids = dict(df.groupby('date')['aid'].apply(set))\n",
    "for date, aIDs in tqdm(sorted(date_aids.items())):\n",
    "\ttotal = Counter()\n",
    "\tfor aID in aIDs:\n",
    "\t\ttotal.update(all_bags[aID])\n",
    "\tdaybags[date] = total\n",
    "len(daybags)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T16:47:34.975136586Z",
     "start_time": "2023-11-19T16:47:23.046866130Z"
    }
   },
   "id": "a931c748153a9ef3"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/413 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dfaa251d84004c1e8e111bf46066ed85"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/413 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "52bf5a7f288f46c4958142958b47ea22"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dayidf = Counter()\n",
    "for date, bag in tqdm(daybags.items()):\n",
    "\tdayidf.update(bag.keys())\n",
    "dayidf = {w: np.log(len(daybags) / f)  for w, f in dayidf.items()}\n",
    "daytotals = {d: sum(bag.values()) for d, bag in daybags.items()}\n",
    "daytfidf = {day: Counter({w: f / daytotals[day] * dayidf[w] for w, f in bag.items()}) for day, bag in tqdm(daybags.items())}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T16:47:41.577842500Z",
     "start_time": "2023-11-19T16:47:34.991008243Z"
    }
   },
   "id": "40bddfa7657bfe5e"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "\n",
    "# def w():\n",
    "def worker_fn(dateidx, target, today_aids, today_tfidf, today_bags):\n",
    "\t\n",
    "\tdatstr = target.strftime('%Y-%m-%d')\n",
    "\tcls_path = Path('/home/fleeb/workspace/local_data/nnn') / 'clusterings' / f'clusters_{datstr}.json'\n",
    "\t\n",
    "\tif cls_path.exists():\n",
    "\t\tprint(f'{dateidx+1}/{413} - Skipping {datstr} - already exists')\n",
    "\t\treturn\n",
    "\t\n",
    "\t# today_aids = date_aids[target]\n",
    "\t# today_bag = daybags[target]\n",
    "\t# today_df = df[df['date'] == target]\n",
    "\t# today_tfidf = token_importances\n",
    "\timportances = today_tfidf\n",
    "\n",
    "\ttreat_bags_as_sets = True\n",
    "\tprior_art_scores = Counter({aID: sum(\n",
    "\t\timportances[w] * (1 if treat_bags_as_sets else f) for w, f in today_bags[aID].items() if w in importances) for aID\n",
    "\t\t\t\t\t\t\t\tin today_aids})\n",
    "\n",
    "\tart_imps = list(prior_art_scores.most_common())\n",
    "\taidorder = np.array([c for c, _ in art_imps])\n",
    "\n",
    "\tart_keywords = {\n",
    "\t\taid: [w for w in sorted(today_bags[aid], key=lambda w: importances[w], reverse=True) if importances[w] > 1e-8] for\n",
    "\t\taid in aidorder}\n",
    "\n",
    "\tfull_clusters = {}\n",
    "\ttodo = aidorder.tolist()\n",
    "\ttotal = len(todo)\n",
    "\t# itr = tqdm(total=total)\n",
    "\t# itr.set_description(f'{dateidx + 1}/{len(daybags)} - {datstr}')\n",
    "\t\n",
    "\tprint(f'{dateidx+1}/{413} - Starting {datstr} ({total} articles)')\n",
    "\t\n",
    "\twhile len(todo):\n",
    "\t\tcenter = todo[0]\n",
    "\t\ttokens, tiers, trace = mainline_cluster_tiers(art_keywords, importances, center, todo, starting_tier=7, min_tier=3, num_kw=200,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t  num_member=200)\n",
    "\t\tprev = len(todo)\n",
    "\t\tfor aids in tiers.values():\n",
    "\t\t\ttodo = [aid for aid in todo if aid not in aids]\n",
    "\t\tif center in todo:\n",
    "\t\t\ttodo.remove(center)\n",
    "\t\t# itr.update(prev - len(todo))\n",
    "\t\tfull_clusters[center] = {'tokens': tokens, 'tiers': tiers, 'trace': trace}\n",
    "\n",
    "\t# itr.close()\n",
    "\n",
    "\tsave_json({center: {'tokens': info['tokens'], 'tiers': {t: list(aids) for t, aids in info['tiers'].items()},\n",
    "\t\t\t\t\t\t'trace': info['trace']} for center, info in full_clusters.items()}, cls_path)\n",
    "\t\n",
    "\tprint(f'{dateidx+1}/{413} - Finished {datstr} ({total} articles) - {len(full_clusters)} clusters')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T16:47:41.648657144Z",
     "start_time": "2023-11-19T16:47:41.583361840Z"
    }
   },
   "id": "10f299c0fd323418"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/413 - Skipping 2020-08-07 - already exists\n",
      "2/413 - Skipping 2020-08-08 - already exists\n",
      "3/413 - Skipping 2020-08-09 - already exists\n",
      "4/413 - Skipping 2020-08-10 - already exists\n",
      "5/413 - Skipping 2020-08-11 - already exists\n",
      "6/413 - Skipping 2020-08-12 - already exists\n",
      "7/413 - Skipping 2020-08-13 - already exists\n",
      "8/413 - Skipping 2020-08-14 - already exists\n",
      "9/413 - Skipping 2020-08-15 - already exists\n",
      "10/413 - Skipping 2020-08-16 - already exists\n",
      "11/413 - Skipping 2020-08-17 - already exists\n",
      "12/413 - Skipping 2020-08-18 - already exists\n",
      "13/413 - Skipping 2020-08-19 - already exists\n",
      "14/413 - Skipping 2020-08-20 - already exists\n",
      "15/413 - Skipping 2020-08-21 - already exists\n",
      "16/413 - Skipping 2020-08-22 - already exists\n",
      "17/413 - Skipping 2020-08-23 - already exists\n",
      "18/413 - Skipping 2020-08-24 - already exists\n",
      "19/413 - Skipping 2020-08-25 - already exists\n",
      "20/413 - Skipping 2020-08-26 - already exists\n",
      "21/413 - Skipping 2020-08-27 - already exists\n",
      "22/413 - Skipping 2020-08-28 - already exists\n",
      "23/413 - Skipping 2020-08-29 - already exists\n",
      "24/413 - Skipping 2020-08-30 - already exists\n",
      "25/413 - Skipping 2020-08-31 - already exists\n",
      "26/413 - Skipping 2020-09-01 - already exists\n",
      "27/413 - Skipping 2020-09-02 - already exists\n",
      "28/413 - Skipping 2020-09-03 - already exists\n",
      "29/413 - Skipping 2020-09-04 - already exists\n",
      "30/413 - Skipping 2020-09-05 - already exists\n",
      "31/413 - Skipping 2020-09-06 - already exists\n",
      "32/413 - Skipping 2020-09-07 - already exists\n",
      "33/413 - Skipping 2020-09-08 - already exists\n",
      "34/413 - Skipping 2020-09-09 - already exists\n",
      "35/413 - Skipping 2020-09-10 - already exists\n",
      "36/413 - Skipping 2020-09-11 - already exists\n",
      "37/413 - Skipping 2020-09-12 - already exists\n",
      "38/413 - Skipping 2020-09-13 - already exists\n",
      "39/413 - Starting 2020-09-14 (14401 articles)\n",
      "40/413 - Skipping 2020-09-15 - already exists\n",
      "41/413 - Skipping 2020-09-16 - already exists\n",
      "42/413 - Skipping 2020-09-17 - already exists\n",
      "43/413 - Skipping 2020-09-18 - already exists\n",
      "44/413 - Skipping 2020-09-19 - already exists\n",
      "45/413 - Skipping 2020-09-20 - already exists\n",
      "46/413 - Skipping 2020-09-21 - already exists\n",
      "47/413 - Skipping 2020-09-22 - already exists\n",
      "48/413 - Skipping 2020-09-23 - already exists\n",
      "49/413 - Skipping 2020-09-24 - already exists\n",
      "50/413 - Skipping 2020-09-25 - already exists\n",
      "51/413 - Skipping 2020-09-26 - already exists\n",
      "52/413 - Starting 2020-09-27 (14811 articles)\n",
      "53/413 - Skipping 2020-09-28 - already exists\n",
      "54/413 - Skipping 2020-09-29 - already exists\n",
      "55/413 - Skipping 2020-09-30 - already exists\n",
      "56/413 - Skipping 2020-10-01 - already exists\n",
      "57/413 - Skipping 2020-10-02 - already exists\n",
      "58/413 - Skipping 2020-10-03 - already exists\n",
      "59/413 - Skipping 2020-10-04 - already exists\n",
      "60/413 - Skipping 2020-10-05 - already exists\n",
      "61/413 - Skipping 2020-10-06 - already exists\n",
      "62/413 - Skipping 2020-10-07 - already exists\n",
      "63/413 - Skipping 2020-10-08 - already exists\n",
      "64/413 - Skipping 2020-10-09 - already exists\n",
      "65/413 - Skipping 2020-10-10 - already exists\n",
      "66/413 - Skipping 2020-10-11 - already exists\n",
      "67/413 - Skipping 2020-10-12 - already exists\n",
      "68/413 - Skipping 2020-10-13 - already exists\n",
      "69/413 - Skipping 2020-10-14 - already exists\n",
      "70/413 - Skipping 2020-10-15 - already exists\n",
      "71/413 - Skipping 2020-10-16 - already exists\n",
      "72/413 - Skipping 2020-10-17 - already exists\n",
      "73/413 - Skipping 2020-10-18 - already exists\n",
      "74/413 - Skipping 2020-10-19 - already exists\n",
      "75/413 - Skipping 2020-10-20 - already exists\n",
      "76/413 - Skipping 2020-10-21 - already exists\n",
      "77/413 - Skipping 2020-10-22 - already exists\n",
      "78/413 - Skipping 2020-10-23 - already exists\n",
      "79/413 - Skipping 2020-10-24 - already exists\n",
      "80/413 - Skipping 2020-10-25 - already exists\n",
      "81/413 - Skipping 2020-10-26 - already exists\n",
      "82/413 - Skipping 2020-10-27 - already exists\n",
      "83/413 - Skipping 2020-10-28 - already exists\n",
      "84/413 - Skipping 2020-10-29 - already exists\n",
      "85/413 - Skipping 2020-10-30 - already exists\n",
      "86/413 - Skipping 2020-10-31 - already exists\n",
      "87/413 - Skipping 2020-11-01 - already exists\n",
      "88/413 - Skipping 2020-11-02 - already exists\n",
      "89/413 - Skipping 2020-11-03 - already exists\n",
      "90/413 - Skipping 2020-11-04 - already exists\n",
      "91/413 - Starting 2020-11-05 (15653 articles)\n",
      "92/413 - Skipping 2020-11-06 - already exists\n",
      "93/413 - Skipping 2020-11-07 - already exists\n",
      "94/413 - Skipping 2020-11-09 - already exists\n",
      "95/413 - Skipping 2020-11-10 - already exists\n",
      "96/413 - Skipping 2020-11-12 - already exists\n",
      "97/413 - Skipping 2020-11-13 - already exists\n",
      "98/413 - Skipping 2020-11-14 - already exists\n",
      "99/413 - Skipping 2020-11-17 - already exists\n",
      "100/413 - Skipping 2020-11-18 - already exists\n",
      "101/413 - Skipping 2020-11-19 - already exists\n",
      "102/413 - Skipping 2020-11-20 - already exists\n",
      "103/413 - Skipping 2020-11-23 - already exists\n",
      "104/413 - Starting 2020-11-24 (15599 articles)\n",
      "105/413 - Skipping 2020-11-28 - already exists\n",
      "106/413 - Skipping 2020-11-29 - already exists\n",
      "107/413 - Skipping 2020-12-01 - already exists\n",
      "108/413 - Starting 2020-12-02 (15656 articles)\n",
      "118/413 - Skipping 2020-12-13 - already exists\n",
      "119/413 - Starting 2020-12-14 (14385 articles)\n",
      "131/413 - Starting 2020-12-31 (14611 articles)\n",
      "144/413 - Starting 2021-01-14 (15557 articles)\n",
      "131/413 - Finished 2020-12-31 (14611 articles) - 12235 clusters\n",
      "132/413 - Starting 2021-01-02 (13776 articles)\n",
      "119/413 - Finished 2020-12-14 (14385 articles) - 11714 clusters\n",
      "120/413 - Starting 2020-12-15 (15002 articles)\n",
      "39/413 - Finished 2020-09-14 (14401 articles) - 11882 clusters\n",
      "157/413 - Skipping 2021-01-28 - already exists\n",
      "158/413 - Skipping 2021-01-29 - already exists\n",
      "159/413 - Starting 2021-01-30 (5653 articles)\n",
      "144/413 - Finished 2021-01-14 (15557 articles) - 12692 clusters\n",
      "145/413 - Starting 2021-01-15 (15903 articles)\n",
      "52/413 - Finished 2020-09-27 (14811 articles) - 12094 clusters\n",
      "170/413 - Starting 2021-02-10 (15729 articles)\n",
      "108/413 - Finished 2020-12-02 (15656 articles) - 12787 clusters\n",
      "109/413 - Starting 2020-12-03 (15815 articles)\n",
      "91/413 - Finished 2020-11-05 (15653 articles) - 12634 clusters\n"
     ]
    }
   ],
   "source": [
    "with Pool(8) as pool:\n",
    "\tpool.starmap(worker_fn, [(i, target, date_aids[target], daytfidf[target], {aid: all_bags[aid] for aid in date_aids[target]}) for i, target in enumerate(sorted(daybags))])\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-11-19T16:47:41.628800163Z"
    }
   },
   "id": "acec7582f9e2f433"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ece36f113f59352f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# for dateidx, target in enumerate(daybags):\n",
    "# \n",
    "# \ttoday_aids = date_aids[target]\n",
    "# \ttoday_bag = daybags[target]\n",
    "# \t# today_df = df[df['date'] == target]\n",
    "# \ttoday_tfidf = daytfidf[target]\n",
    "# \timportances = today_tfidf\n",
    "# \n",
    "# \ttreat_bags_as_sets = True\n",
    "# \tprior_art_scores = Counter({aID: sum(\n",
    "# \t\timportances[w] * (1 if treat_bags_as_sets else f) for w, f in all_bags[aID].items() if w in importances) for aID\n",
    "# \t\t\t\t\t\t\t\tin today_aids})\n",
    "# \n",
    "# \n",
    "# \tdef article_affinity(aid1, aid2):\n",
    "# \t\tbag1, bag2 = all_bags[aid1], all_bags[aid2]\n",
    "# \t\treturn sum(importances[w] * (1 if treat_bags_as_sets else min(f, bag2[w])) for w, f in bag1.items() if\n",
    "# \t\t\t\t   w in bag2) / np.sqrt(prior_art_scores[aid1] * prior_art_scores[aid2])\n",
    "# \n",
    "# \n",
    "# \tart_imps = list(prior_art_scores.most_common())\n",
    "# \taidorder = np.array([c for c, _ in art_imps])\n",
    "# \n",
    "# \tart_keywords = {\n",
    "# \t\taid: [w for w in sorted(all_bags[aid], key=lambda w: importances[w], reverse=True) if importances[w] > 1e-8] for\n",
    "# \t\taid in aidorder}\n",
    "# \n",
    "# \tdatstr = target.strftime('%Y-%m-%d')\n",
    "# \n",
    "# \tfull_clusters = {}\n",
    "# \ttodo = aidorder.tolist()\n",
    "# \ttotal = len(todo)\n",
    "# \titr = tqdm(total=total)\n",
    "# \titr.set_description(f'{dateidx + 1}/{len(daybags)} - {datstr}')\n",
    "# \twhile len(todo):\n",
    "# \t\tcenter = todo[0]\n",
    "# \t\ttokens, tiers, trace = mainline_cluster_tiers(center, todo, starting_tier=7, min_tier=3, num_kw=200,\n",
    "# \t\t\t\t\t\t\t\t\t\t\t\t\t  num_member=200)\n",
    "# \t\tprev = len(todo)\n",
    "# \t\tfor aids in tiers.values():\n",
    "# \t\t\ttodo = [aid for aid in todo if aid not in aids]\n",
    "# \t\tif center in todo:\n",
    "# \t\t\ttodo.remove(center)\n",
    "# \t\titr.update(prev - len(todo))\n",
    "# \t\tfull_clusters[center] = {'tokens': tokens, 'tiers': tiers, 'trace': trace}\n",
    "# \n",
    "# \titr.close()\n",
    "# \n",
    "# \tsave_json({center: {'tokens': info['tokens'], 'tiers': {t: list(aids) for t, aids in info['tiers'].items()},\n",
    "# \t\t\t\t\t\t'trace': info['trace']} for center, info in full_clusters.items()},\n",
    "# \t\t\t  cluster_root / f'clusters_{datstr}.json')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T16:05:03.473038122Z",
     "start_time": "2023-11-14T16:05:03.472843816Z"
    }
   },
   "id": "e8c2f7ad8c47d2f1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T16:05:03.476355333Z",
     "start_time": "2023-11-14T16:05:03.472956661Z"
    }
   },
   "id": "fe8bccfa8bf29570"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d87f6c24d562a844"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
