{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-02T15:36:15.959671634Z",
     "start_time": "2023-11-02T15:36:15.851281678Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "from tabulate import tabulate\n",
    "import random, json\n",
    "import pycountry\n",
    "from iso639 import languages\n",
    "import networkx as nx\n",
    "from dateutil import parser\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from langdetect import detect\n",
    "from omnibelt import load_json, save_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "54"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang_clusters = {'en': ['au', 'ca', 'gb', 'ie', 'in', 'my', 'ng', 'nz', 'ph', 'sa', 'sg', 'us', 'za'],\n",
    "\t\t\t\t 'es': ['ar', 'co', 'cu', 'mx', 've'], 'de': ['at', 'ch', 'de'], 'fr': ['be', 'fr', 'ma'],\n",
    "\t\t\t\t 'zh': ['cn', 'hk', 'tw'], 'ar': ['ae', 'eg'], 'pt': ['br', 'pt'], 'bg': ['bg'], 'cs': ['cz'],\n",
    "\t\t\t\t 'el': ['gr'], 'he': ['il'], 'hu': ['hu'], 'id': ['id'], 'it': ['it'], 'ja': ['jp'], 'ko': ['kr'],\n",
    "\t\t\t\t 'lt': ['lt'], 'lv': ['lv'], 'nl': ['nl'], 'no': ['no'], 'pl': ['pl'], 'ro': ['ro'], 'ru': ['ru'],\n",
    "\t\t\t\t 'sv': ['se'], 'sl': ['si'], 'sk': ['sk'], 'sr': ['rs'], 'th': ['th'], 'tr': ['tr'], 'uk': ['ua']}\n",
    "loc_names = {'gb': 'United Kingdom', 'ar': 'Argentina', 'pl': 'Poland', 'sk': 'Slovakia', 'us': 'United States',\n",
    "\t\t\t 'eg': 'Egypt', 'no': 'Norway', 'ph': 'Philippines', 'at': 'Austria', 'rs': 'Serbia', 'tw': 'Taiwan',\n",
    "\t\t\t 'be': 'Belgium', 'cu': 'Cuba', 'sa': 'Saudi Arabia', 'th': 'Thailand', 'id': 'Indonesia',\n",
    "\t\t\t 'ru': 'Russian Federation', 'ch': 'Switzerland', 'fr': 'France', 'lt': 'Lithuania', 'tr': 'Turkey',\n",
    "\t\t\t 'de': 'Germany', 'cz': 'Czechia', 'pt': 'Portugal', 'ae': 'United Arab Emirates', 'it': 'Italy',\n",
    "\t\t\t 'cn': 'China', 'lv': 'Latvia', 'nl': 'Netherlands', 'hk': 'Hong Kong', 'ca': 'Canada', 'br': 'Brazil',\n",
    "\t\t\t 'hu': 'Hungary', 'kr': 'Korea', 'si': 'Slovenia', 'au': 'Australia', 'my': 'Malaysia', 'ie': 'Ireland',\n",
    "\t\t\t 'ua': 'Ukraine', 'in': 'India', 'ma': 'Morocco', 'bg': 'Bulgaria', 'ng': 'Nigeria', 'il': 'Israel',\n",
    "\t\t\t 'se': 'Sweden', 'za': 'South Africa', 've': 'Venezuela', 'nz': 'New Zealand', 'jp': 'Japan',\n",
    "\t\t\t 'sg': 'Singapore', 'gr': 'Greece', 'mx': 'Mexico', 'co': 'Colombia', 'ro': 'Romania'}\n",
    "lang_names = {'en': 'English', 'ko': 'Korean', 'ru': 'Russian', 'es': 'Spanish', 'pt': 'Portuguese', 'cs': 'Czech',\n",
    "\t\t\t  'tr': 'Turkish', 'nl': 'Dutch', 'ar': 'Arabic', 'fr': 'French', 'bg': 'Bulgarian', 'id': 'Indonesian',\n",
    "\t\t\t  'sk': 'Slovak', 'el': 'Greek', 'he': 'Hebrew', 'sr': 'Serbian', 'hu': 'Hungarian', 'th': 'Thai',\n",
    "\t\t\t  'zh': 'Chinese', 'no': 'Norwegian', 'sl': 'Slovenian', 'sv': 'Swedish', 'de': 'German', 'lv': 'Latvian',\n",
    "\t\t\t  'pl': 'Polish', 'it': 'Italian', 'ro': 'Romanian', 'lt': 'Lithuanian', 'ja': 'Japanese',\n",
    "\t\t\t  'uk': 'Ukrainian'}\n",
    "cluster_id = {loc: lang for lang, locs in lang_clusters.items() for loc in locs}\n",
    "len(cluster_id)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T15:36:16.343559566Z",
     "start_time": "2023-11-02T15:36:16.293228839Z"
    }
   },
   "id": "2aaed80af771fb0f"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def show_date(date):\n",
    "\treturn date.strftime('%d %b%y')\n",
    "def get_locs(article):\n",
    "\treturn [f'{loc_names[loc]}' for loc in sorted(set(i['location'] for i in article['instances']))]\n",
    "def get_cats(article):\n",
    "\treturn [f'<{cat}>' for cat in sorted(set(i['category'] for i in article['instances']))]\n",
    "def view_article(art, detailed=False):\n",
    "\tcats = ' '.join(get_cats(art))\n",
    "\tlocs = ', '.join(map(repr, get_locs(art)))\n",
    "\tpublished = parser.parse(art['publishedAt'])\n",
    "\tcollected = [parser.parse(i['collectedAt']) for i in art['instances']]\n",
    "\tfirst = min(collected)\n",
    "\tlast = max(collected)\n",
    "\ttiming = f'{show_date(first)}' if first == last else f'{show_date(first)} - {show_date(last)}'\n",
    "\n",
    "\tlines = []\n",
    "\tif 'en-title' in art:\n",
    "\t\tlines.append(f'English Title: {art[\"en-title\"]!r}')\n",
    "\tlines.append(f'{lang_names[art[\"language\"]]} Title: {art[\"title\"]!r}')\n",
    "\tlines.append(f'Categories: {cats}  ---   {locs} ({timing})')\n",
    "\n",
    "\tif detailed:\n",
    "\t\tadded_desc = False\n",
    "\t\tdesc = art.get('description')\n",
    "\t\ten_desc = art.get('en-description')\n",
    "\t\tif en_desc is not None and len(en_desc):\n",
    "\t\t\tlines.append(f'English Description: {en_desc}')\n",
    "\t\t\tadded_desc = True\n",
    "\t\tif desc is not None and len(desc):\n",
    "\t\t\tlines.append(f'{lang_names[art[\"language\"]]} Description: {desc}')\n",
    "\t\t\tadded_desc = True\n",
    "\t\tif not added_desc:\n",
    "\t\t\tlines.append(f'- No description -')\n",
    "\tprint('\\n'.join(lines))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T15:36:17.349756915Z",
     "start_time": "2023-11-02T15:36:17.301762366Z"
    }
   },
   "id": "f3328ed774ca9a7a"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/54 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c18454a0d8e04e589090e16feb98c032"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "4719199"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root = Path('/home/fleeb/workspace/local_data/nnn')\n",
    "recs = (root / 'old-bb-v1').glob('**/*.json')\n",
    "recs = (root / 'babel-briefings-v1').glob('**/*.json')\n",
    "recs = list(recs)\n",
    "len(recs)\n",
    "articles = []\n",
    "for rec in tqdm(recs):\n",
    "\tarticles.extend(load_json(rec))\n",
    "len(articles)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T15:37:04.926770362Z",
     "start_time": "2023-11-02T15:36:18.099898967Z"
    }
   },
   "id": "40a5912898e5088"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/4719199 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e998e2b39b5144dba6a6233de95945de"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "(54, 30)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "by_loc = {}\n",
    "by_lang = {}\n",
    "for article in tqdm(articles):\n",
    "\t# article['published'] = parser.parse(article['publishedAt'])\n",
    "\tby_lang.setdefault(article['language'], []).append(article)\n",
    "\tfor instance in article['instances']:\n",
    "\t\t# instance['collected'] = parser.parse(instance['collectedAt'])\n",
    "\t\tby_loc.setdefault(instance['location'], []).append(article)\n",
    "len(by_loc), len(by_lang)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T15:37:07.161201531Z",
     "start_time": "2023-11-02T15:37:04.926996509Z"
    }
   },
   "id": "885dfa17cff11bd3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "99314156f236449c"
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from rake_nltk import Rake\n",
    "import spacy\n",
    "# from sklearn.cluster import DBSCAN\n",
    "# import numpy as np\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "# rake_nltk_var = Rake()\n",
    "# lemmatizer = WordNetLemmatizer()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T16:53:07.895082650Z",
     "start_time": "2023-11-02T16:53:07.007159167Z"
    }
   },
   "id": "ad5e600c6d89dff5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "noun_phrases = []\n",
    "for headline in headlines:\n",
    "    doc = nlp(headline)\n",
    "    for p in doc.noun_chunks:\n",
    "        noun_phrases.append(str(p))\n",
    "\n",
    "# Convert noun phrases to vectors\n",
    "vectors = [nlp(p).vector for p in noun_phrases]\n",
    "\n",
    "# Apply DBSCAN clustering\n",
    "clustering = DBSCAN(eps=0.5, min_samples=1, metric='cosine').fit(np.array(vectors))\n",
    "labels = clustering.labels_\n",
    "\n",
    "# Group noun phrases based on clustering results\n",
    "groups = {}\n",
    "for np, label in zip(noun_phrases, labels):\n",
    "    if label not in groups:\n",
    "        groups[label] = []\n",
    "    groups[label].append(np)\n",
    "\n",
    "# Print the grouped noun phrases\n",
    "for label, nps in groups.items():\n",
    "    print(f\"Group {label}: {nps}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cf44f02535b1907a"
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "data": {
      "text/plain": "259718"
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = by_lang['de']\n",
    "len(batch)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T17:04:10.750747570Z",
     "start_time": "2023-11-02T17:04:10.700598844Z"
    }
   },
   "id": "a6023af18feaf95f"
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Title: 'iOS 14.2 is here: This change will make you happy - inside digital'\n",
      "German Title: 'iOS 14.2 ist da: Diese Änderung wird dich glücklich machen - inside digital'\n",
      "Categories: <technology>  ---   'Germany' (07 Nov20)\n",
      "English Description: It is now possible to use iPhones with iOS 14.2. We'll tell you what's changing with the new version. It's a lot.\n",
      "German Description: Ab sofort ist es möglich, iPhones mit iOS 14.2 zu nutzen. Wir verraten dir, was sich mit der neuen Version alles ändert. Es ist einiges.\n",
      "inside digital\n"
     ]
    }
   ],
   "source": [
    "art = random.choice(batch)\n",
    "view_article(art, detailed=True)\n",
    "print(art['source-name'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T17:04:14.780446657Z",
     "start_time": "2023-11-02T17:04:14.728898093Z"
    }
   },
   "id": "344957ccf1296321"
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "data": {
      "text/plain": "1437"
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sources = {}\n",
    "for art in batch:\n",
    "\tsources.setdefault(art['source-name'], []).append(art)\n",
    "len(sources)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T17:07:43.597259380Z",
     "start_time": "2023-11-02T17:07:43.554885002Z"
    }
   },
   "id": "6e986750617e3da1"
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [
    {
     "data": {
      "text/plain": "('Www.otz.de', 5)"
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src, arts = random.choice(list(sources.items()))\n",
    "src, len(arts)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T17:35:11.576677705Z",
     "start_time": "2023-11-02T17:35:11.552866219Z"
    }
   },
   "id": "588e7282bd9679f6"
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [
    {
     "data": {
      "text/plain": "[['Ex-basketball player Dirk Nowitzki extends contract with Thüringer Bauerfeind AG',\n  'Ostthüringer Zeitung'],\n ['No stadium solution for Viktoria Berlin: FC Carl Zeiss Jena as a beneficiary?',\n  'Ostthüringer Zeitung'],\n ['Thuringian State Observatory measures over 8,000 potential threats to the Earth',\n  'Ostthüringer Zeitung'],\n ['Participants from Jena wanted for study on corona consequences',\n  'Thuringian state newspaper'],\n ['Greiz district: Introduced viruses are becoming more common',\n  'Ostthüringer Zeitung']]"
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segs = [a.get('en-title', art['title']).split(' - ') for a in arts]\n",
    "segs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T17:35:11.791495509Z",
     "start_time": "2023-11-02T17:35:11.752771465Z"
    }
   },
   "id": "9ea785c976242f37"
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [
    {
     "data": {
      "text/plain": "['ORF.at',\n 'ORF.at',\n 'ORF.at',\n 'ORF.at',\n 'ORF.at',\n 'ORF.at',\n 'ORF.at',\n 'ORF.at',\n 'ORF.at',\n 'ORF.at',\n 'Gamereactor Deutschland',\n 'ORF.at',\n 'ORF.at',\n 'ORF.at',\n 'ORF.at',\n 'ORF.at',\n 'ORF.at',\n 'ORF.at',\n 'ORF.at',\n 'ORF.at',\n 'ORF.at',\n 'ORF.at',\n 'ORF.at',\n 'ORF.at',\n 'ORF.at',\n 'ORF.at',\n 'ORF.at',\n 'ORF.at',\n 'ORF.at',\n 'ORF.at',\n 'ORF.at',\n 'ORF.at',\n 'ORF.at',\n 'ORF.at']"
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endterms = [x.get('en-title', art['title']).split(' - ')[-1] for x in xs]\n",
    "endterms"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T17:09:47.519895339Z",
     "start_time": "2023-11-02T17:09:47.475773959Z"
    }
   },
   "id": "caeb58b6eef9a2f0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "fe5f4a8e986b8b3c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def preprocess_title(art):\n",
    "\t\n",
    "\ttitle = art.get('en-title', art['title'])\n",
    "\t\n",
    "\t\n",
    "\t\n",
    "\tpass"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7dba9ab21eb4817e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b591ae24f89a4fb6"
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "def to_prompt(art):\n",
    "\ttitle = art.get('en-title', art['title'])\n",
    "\t# rake_nltk_var.extract_keywords_from_text(title)\n",
    "\tdesc = art.get('en-description', art['description'])\n",
    "\tif desc is not None and len(desc):\n",
    "\t\tpass\n",
    "\t\t# rake_nltk_var.extract_keywords_from_text(desc)\n",
    "\telse:\n",
    "\t\tdesc = ''\n",
    "\tcontent = art.get('en-content', art['content'])\n",
    "\tif content is not None and len(content):\n",
    "\t\tpass\n",
    "\t\t# rake_nltk_var.extract_keywords_from_text(content)\n",
    "\telse:\n",
    "\t\tcontent = ''\n",
    "\t\n",
    "\ttext = f'{title}\\n{desc}\\n{content}'\n",
    "\treturn text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T16:12:58.342146964Z",
     "start_time": "2023-11-02T16:12:58.296433013Z"
    }
   },
   "id": "146f64677a03387e"
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "text = to_prompt(art)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T16:57:41.036190113Z",
     "start_time": "2023-11-02T16:57:40.950452905Z"
    }
   },
   "id": "963292d87e1a235b"
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T16:57:45.984851782Z",
     "start_time": "2023-11-02T16:57:45.917486411Z"
    }
   },
   "id": "74902c58e5b19979"
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['argentina', \"'s\", 'nuclear', 'power', 'plant', 'achieve', 'record', 'electricity', 'production', '-', 'fact', '.', 'bg', '\\n', 'company', 'operator', 'argentine', 'nuclear', 'power', 'plant', 'nucleoéléctrica', 'argentina', '..', '\\n', 'nuclear', 'power', 'argentina', '2020', ',', 'grupo', 'la', 'provincia', '.', '„', '\"', '1', '2', '„', '\"', '7,947', '1', '-', '30', '.', ',', '2020', '.', ',', ',', '5,059', '.', '„', ',', '\"', ',', '.']\n"
     ]
    }
   ],
   "source": [
    "lemmatized_tokens = [token.lemma_.lower() for token in doc if str(token).lower() not in stopwords.words('english')]\n",
    "print(lemmatized_tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T16:59:35.815902962Z",
     "start_time": "2023-11-02T16:59:35.773744121Z"
    }
   },
   "id": "ee8ef83151121067"
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "data": {
      "text/plain": "[Argentina's nuclear power plants,\n a record,\n electricity production - Facts,\n BG\n The company operator,\n the Argentine nuclear power plants,\n Nucleoéléctrica Argentina,\n Nuclear Power Argentina,\n Grupo La Provincia]"
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(doc.noun_chunks)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T17:01:05.760619929Z",
     "start_time": "2023-11-02T17:01:05.704554809Z"
    }
   },
   "id": "d46fa056ce4b9932"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "doc.no"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c242417f51307e95"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "58d48849417a6a25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8f77999dca2c1db8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8b9b68f4b4f5a44c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "be42159d3bde84bb"
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Argentina's nuclear power plants achieved a record in electricity production - Facts.BG\n",
      "The company operator of the Argentine nuclear power plants Nucleoéléctrica Argentina ..\n",
      "Nuclear Power Argentina 2020, Grupo La Provincia. „“ 1 2 „“ 7,947 1 - 30 . , 2020 . , , 5,059 . „, “, .\n",
      "---------------  -\n",
      "Argentina        3\n",
      "power            2\n",
      "plant            2\n",
      "„                2\n",
      "record           1\n",
      "electricity      1\n",
      "production       1\n",
      "Facts.BG         1\n",
      "company          1\n",
      "operator         1\n",
      "Argentine        1\n",
      "Nucleoéléctrica  1\n",
      "..               1\n",
      "Nuclear          1\n",
      "Power            1\n",
      "Grupo            1\n",
      "La               1\n",
      "Provincia        1\n",
      "“                1\n",
      "---------------  -\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text = to_prompt(art)\n",
    "# rake_nltk_var.extract_keywords_from_text(text)\n",
    "# keywords = rake_nltk_var.get_ranked_phrases()\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "tokens = [word for word in tokens if word.lower() not in stopwords.words('english')]\n",
    "\n",
    "# wordcloud = WordCloud(width = 800, height = 800, background_color ='white', min_font_size = 10).generate(' '.join(tokens))\n",
    "# keywords = wordcloud.words_\n",
    "\n",
    "# # tokens = word_tokenize(text.lower())  # Lowercasing\n",
    "# tokens = word_tokenize(text)\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "tags = pos_tag(lemmatized_tokens)\n",
    "keywords = [word for word, pos in tags if pos in ['NN', 'NNP']]\n",
    "\n",
    "keyword_freq = Counter(keywords)\n",
    "# print(f'Input: {title!r}')\n",
    "print(text)\n",
    "print(tabulate(keyword_freq.most_common()))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T16:46:24.180394409Z",
     "start_time": "2023-11-02T16:46:24.123434324Z"
    }
   },
   "id": "dfbe6f4a2c8780fd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7d9d5dd795b61606"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "['today']"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rake_nltk_var.extract_keywords_from_text(art['source-name'])\n",
    "rake_nltk_var.get_ranked_phrases()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T15:45:24.085002870Z",
     "start_time": "2023-11-02T15:45:24.016442976Z"
    }
   },
   "id": "b55f9764a23b646"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/fleeb/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/fleeb/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/fleeb/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "sentence = \"Apple is looking at buying U.K. startup for $1 billion\"\n",
    "tokens = word_tokenize(sentence)\n",
    "tags = pos_tag(tokens)\n",
    "tree = ne_chunk(tags)\n",
    "keywords = [leaf for leaf in tree if isinstance(leaf, nltk.Tree)]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T15:53:14.130031362Z",
     "start_time": "2023-11-02T15:53:13.810273599Z"
    }
   },
   "id": "7f2f5fe76c6c9b7f"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "[Tree('GPE', [('Apple', 'NNP')])]"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T15:53:21.257383587Z",
     "start_time": "2023-11-02T15:53:21.191848236Z"
    }
   },
   "id": "af43963870e5ab48"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/fleeb/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "text = \"This is a sample sentence, showing off the stop words filtration.\"\n",
    "tokens = word_tokenize(text)\n",
    "fdist = FreqDist(tokens)\n",
    "keywords = [word for word, count in fdist.items() if count > 1]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T15:56:34.927314537Z",
     "start_time": "2023-11-02T15:56:34.897711764Z"
    }
   },
   "id": "b7686fc285a05927"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "[]"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T15:56:35.327043270Z",
     "start_time": "2023-11-02T15:56:35.251117633Z"
    }
   },
   "id": "193b1c810321fbc8"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/fleeb/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "['brown', 'fox', 'dog']"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "text = \"The quick brown fox jumps over the lazy dog\"\n",
    "tokens = word_tokenize(text)\n",
    "tags = pos_tag(tokens)\n",
    "keywords = [word for word, pos in tags if pos in ['NN', 'NNP']]\n",
    "keywords"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T15:57:16.083880385Z",
     "start_time": "2023-11-02T15:57:16.038694263Z"
    }
   },
   "id": "d38196b597571a28"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/fleeb/nltk_data...\n",
      "[nltk_data] Downloading package punkt to /home/fleeb/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T15:38:35.955936166Z",
     "start_time": "2023-11-02T15:38:32.257926236Z"
    }
   },
   "id": "bd142ab40cd4efac"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "from rake_nltk import Rake\n",
    "\n",
    "rake_nltk_var = Rake()\n",
    "text = \"This is a sample sentence, showing off the stop words filtration.\"\n",
    "rake_nltk_var.extract_keywords_from_text(text)\n",
    "keyword_ranked_phrases = rake_nltk_var.get_ranked_phrases()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T15:40:18.225194546Z",
     "start_time": "2023-11-02T15:40:18.175791239Z"
    }
   },
   "id": "ec8e94972557711"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "['stop words filtration', 'sample sentence', 'showing']"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword_ranked_phrases"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T15:40:23.975406112Z",
     "start_time": "2023-11-02T15:40:23.942354721Z"
    }
   },
   "id": "c3f3921fb319c42d"
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([['hits', 'high', 'lawsuits', 'global', 'giants', 'face',\n",
      "        'climate', 'change', 'challenge', 'antitrust']], dtype=object)]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample headlines\n",
    "headlines = [\n",
    "    \"Climate Change: A Global Challenge\",\n",
    "    \"Tech Giants Face Antitrust Lawsuits\",\n",
    "    \"Stock Market Hits Record High\"\n",
    "]\n",
    "\n",
    "# Preprocess text\n",
    "preprocessed_headlines = [headline.lower() for headline in headlines]\n",
    "\n",
    "# Create TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(max_df=0.85, max_features=10, stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(preprocessed_headlines)\n",
    "\n",
    "# Extract keywords\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "keywords = [feature_names[idx] for idx in tfidf_matrix.sum(axis=0).argsort()[0, ::-1]]\n",
    "print(keywords)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T16:25:29.197647450Z",
     "start_time": "2023-11-02T16:25:29.154630138Z"
    }
   },
   "id": "5e010c2cde5ce9df"
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T16:38:20.381525776Z",
     "start_time": "2023-11-02T16:38:18.778950804Z"
    }
   },
   "id": "582ad0ac5cda786a"
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "data": {
      "text/plain": "Argentina's nuclear power plants achieved a record in electricity production - Facts.BG\nThe company operator of the Argentine nuclear power plants Nucleoéléctrica Argentina ..\nNuclear Power Argentina 2020, Grupo La Provincia. „“ 1 2 „“ 7,947 1 - 30 . , 2020 . , , 5,059 . „, “, ."
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "# lemmatizer = nlp.add_pipe(\"lemmatizer\")\n",
    "# This usually happens under the hood\n",
    "# processed = lemmatizer(doc)\n",
    "# processed\n",
    "doc"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T16:39:26.213371872Z",
     "start_time": "2023-11-02T16:39:26.156236760Z"
    }
   },
   "id": "d611e09a3c2b8111"
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "data": {
      "text/plain": "['argentina',\n \"'s\",\n 'nuclear',\n 'power',\n 'plant',\n 'achieve',\n 'record',\n 'electricity',\n 'production',\n '-',\n 'fact',\n '.',\n 'bg',\n '\\n',\n 'company',\n 'operator',\n 'argentine',\n 'nuclear',\n 'power',\n 'plant',\n 'nucleoéléctrica',\n 'argentina',\n '..',\n '\\n',\n 'nuclear',\n 'power',\n 'argentina',\n '2020',\n ',',\n 'grupo',\n 'la',\n 'provincia',\n '.',\n '„',\n '\"',\n '1',\n '2',\n '„',\n '\"',\n '7,947',\n '1',\n '-',\n '30',\n '.',\n ',',\n '2020',\n '.',\n ',',\n ',',\n '5,059',\n '.',\n '„',\n ',',\n '\"',\n ',',\n '.']"
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized_tokens = [token.lemma_.lower() for token in doc if str(token).lower() not in stopwords.words('english')]\n",
    "lemmatized_tokens"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T16:42:35.508628671Z",
     "start_time": "2023-11-02T16:42:35.453283407Z"
    }
   },
   "id": "9c1aa97e2e3f9d74"
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "data": {
      "text/plain": "'Argentina'"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T16:41:54.732237043Z",
     "start_time": "2023-11-02T16:41:54.701254567Z"
    }
   },
   "id": "1968b6035006a46f"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Example text\n",
    "text = \"Barack Obama, President Obama, and Obama are all referring to the same person.\"\n",
    "\n",
    "# Tokenize and lemmatize\n",
    "tokens = word_tokenize(text.lower())  # Lowercasing\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "# Now 'Barack Obama', 'President Obama', and 'Obama' can be processed further for similarity matching or synonym mapping\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T15:38:40.178690Z",
     "start_time": "2023-11-02T15:38:39.666855407Z"
    }
   },
   "id": "ba3335c606ba506"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "['barack',\n 'obama',\n ',',\n 'president',\n 'obama',\n ',',\n 'and',\n 'obama',\n 'are',\n 'all',\n 'referring',\n 'to',\n 'the',\n 'same',\n 'person',\n '.']"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized_tokens"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T15:38:44.379334150Z",
     "start_time": "2023-11-02T15:38:44.205418852Z"
    }
   },
   "id": "f285357e16b33185"
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 0: ['Apple', 'new iPhone', 'Microsoft', 'Windows', 'Windows', 'new features', 'Apple iPhone sales skyrocket']\n",
      "Group 1: ['New iPhone']\n",
      "Group 2: ['positive reviews']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "\n",
    "# Load SpaCy NLP model\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Example dataset of news headlines\n",
    "headlines = [\n",
    "    \"Apple announces new iPhone\",\n",
    "    \"Microsoft releases Windows 11\",\n",
    "    \"New iPhone receives positive reviews\",\n",
    "    \"Windows 11 has new features\",\n",
    "    \"Apple iPhone sales skyrocket\",\n",
    "]\n",
    "\n",
    "# Preprocess headlines and extract noun phrases\n",
    "noun_phrases = []\n",
    "for headline in headlines:\n",
    "    doc = nlp(headline)\n",
    "    for p in doc.noun_chunks:\n",
    "        noun_phrases.append(str(p))\n",
    "\n",
    "# Convert noun phrases to vectors\n",
    "vectors = [nlp(p).vector for p in noun_phrases]\n",
    "\n",
    "# Apply DBSCAN clustering\n",
    "clustering = DBSCAN(eps=0.5, min_samples=1, metric='cosine').fit(np.array(vectors))\n",
    "labels = clustering.labels_\n",
    "\n",
    "# Group noun phrases based on clustering results\n",
    "groups = {}\n",
    "for np, label in zip(noun_phrases, labels):\n",
    "    if label not in groups:\n",
    "        groups[label] = []\n",
    "    groups[label].append(np)\n",
    "\n",
    "# Print the grouped noun phrases\n",
    "for label, nps in groups.items():\n",
    "    print(f\"Group {label}: {nps}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T16:52:02.691927575Z",
     "start_time": "2023-11-02T16:52:01.760943419Z"
    }
   },
   "id": "738ef2d7613d5c73"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ad91d0af77a60424"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8aa3a48511da59f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
